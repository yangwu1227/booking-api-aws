{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Booking Service API","text":"<p>An asynchronous RESTful API built with Python, FastAPI, and Docker for learning purposes. The application enables users to submit talk booking requests and provides speakers with endpoints to manage these requests via the API. It integrates with a PostgreSQL database hosted on AWS RDS and is containerized using Docker for both local development and deployment on AWS ECS Fargate.</p> <p>The API includes the following features:</p> <ul> <li>Submit booking requests: Users can submit requests for talks.</li> <li>Retrieve booking requests: Speakers can view all submitted booking requests.</li> <li>Update booking requests by ID: Speakers can accept or reject booking requests based on their ID.</li> <li>Delete booking requests by ID: Speakers can remove booking requests using their ID.</li> </ul>"},{"location":"application_overview/","title":"Application Overview","text":""},{"location":"application_overview/#app-package-app","title":"App Package (<code>app</code>)","text":"<p> Sub-Module Description api Contains API endpoint implementations such as submission, acceptance, rejection, deletion, and listing of booking requests. models Holds the application\u2019s data models for the database (SQLAlchemy ORM) and Pydantic models used for validation and serialization. auth.py Implements authentication mechanisms via OAuth2, including password hashing, token creation/validation, and user role management. config.py Manages the application\u2019s configuration settings, fetching database URLs from AWS Secrets Manager. db.py Provides database connection and session management using SQLAlchemy. This includes database session creation and dependency injections for FastAPI. main.py The entry point for the application, configuring the FastAPI instance and routing API endpoints. <p></p>"},{"location":"application_overview/#database-db","title":"Database (<code>db</code>)","text":"<p> File Description create_db.sql SQL script for initializing the database schema, used specifically for local development and testing. It sets up the database for a <code>test-db</code> service as defined in <code>db.Dockerfile</code>. The script is added to the Docker container via <code>ADD db/create_db.sql /docker-entrypoint-initdb.d</code> to automatically run upon container initialization. <p></p>"},{"location":"application_overview/#docker-files-docker","title":"Docker Files (<code>docker</code>)","text":"<p> File Description db.Dockerfile Dockerfile for setting up the test database on container start. deploy.Dockerfile Configures the deployment environment for running the application in ECS. test.Dockerfile Sets up the environment for running tests using <code>pytest</code> within a container. <p></p>"},{"location":"application_overview/#migrations-migrations","title":"Migrations (<code>migrations</code>)","text":"<p> File/Folder Description alembic.ini Configuration file for Alembic, handling database migrations. versions Contains versioned migration scripts for schema evolution, ensuring the database remains up-to-date. <p></p>"},{"location":"application_overview/#automation-scripts-scripts","title":"Automation Scripts (<code>scripts</code>)","text":"<p> Script Description entrypoint.sh Script executed when the local development and test container starts, initializes application services. generate_and_store_keys.sh Script for generating and storing keys using AWS Secrets Manager. run_black_isort.sh Executes linting and formatting commands (<code>black</code> and <code>isort</code>). <p></p>"},{"location":"application_overview/#tools-tools","title":"Tools (<code>tools</code>)","text":"<p> Tool Description deploy_ecs.py Automates the registration of new task definitions using boto3, executed by the <code>.github/workflows/ecr_ecs.yml</code> reusable workflow for both <code>dev</code> and <code>prod</code> environments. It also launches a standalone container to verify if data migrations need to be run; the <code>migrations</code> directory is accessible inside the container. manage_passwords.py Handles password rotation, securely storing secrets in AWS Secrets Manager and updating the database with new credentials. manage_passwords_trigger.py Orchestrates password rotation by launching a standalone Fargate task that inserts or updates user credentials in the <code>users</code> table. <p></p>"},{"location":"application_overview/#authentication-authpy","title":"Authentication (<code>auth.py</code>)","text":"<p> Functionality Description Token Creation Creates JWT tokens for user sessions, including role-based and time-based restrictions. Password Hashing Uses <code>bcrypt</code> for securely hashing and verifying user passwords. User Authentication Verifies user credentials against stored values in the database and authenticates users with JWT tokens. Role Management Validates user roles (e.g., admin, requester) and enforces access control based on these roles. <p></p>"},{"location":"application_overview/#database-management-dbpy","title":"Database Management (<code>db.py</code>)","text":"<p> Functionality Description Session Management Provides SQLAlchemy session creation, ensuring connections are managed efficiently across API requests. Dependency Injection Integrates sessions with FastAPI\u2019s dependency system for use in endpoints. <p></p>"},{"location":"application_overview/#configuration-management-configpy","title":"Configuration Management (<code>config.py</code>)","text":"<p> Setting/Attribute Description environment Identifies the environment the application is running in (<code>dev</code>, <code>prod</code>, or <code>test</code>). database_url Fetches the database connection string from AWS Secrets Manager, ensuring secure access. debug/testing flags Controls whether the application runs in debug or testing mode. <p></p>"},{"location":"application_overview/#api-endpoints-api","title":"API Endpoints (<code>api</code>)","text":"<p> Endpoint Description /submit_request Allows submission of new booking requests. /list_requests Lists all booking requests. Restricted to admin users. /accept_request Accepts a specific booking request by ID. Restricted to admin users. /reject_request Rejects a specific booking request by ID. Restricted to admin users. /delete_request/{id} Deletes a booking request by ID. Restricted to admin users. /ping/ Health check endpoint that verifies database migration status. Ensures the system is up-to-date before deployment. <p></p>"},{"location":"application_overview/#sqlalchemy-orm-pydantic-models","title":"SQLAlchemy ORM &amp; Pydantic Models","text":""},{"location":"application_overview/#database-models-modelsdb_modelspy","title":"Database Models (<code>models/db_models.py</code>)","text":"<p> Model Description User Represents users, storing their username, hashed password, role, and status (active/disabled). Booking Represents a booking request, storing event details, duration, status, and requestor email. <p></p>"},{"location":"application_overview/#pydantic-models-modelspydantic_modelspy","title":"Pydantic Models (<code>models/pydantic_models.py</code>)","text":"<p> Model Description Address Represents event addresses, including validation for street, city, state, and country fields. RequestStatus Enum for status values: <code>pending</code>, <code>accepted</code>, <code>rejected</code>. BookingResponse Pydantic model representing booking details, including validation for event time, topic, and address. SubmissionRequest Model for incoming booking submissions, used for validation in API endpoints. <p></p>"},{"location":"authentication/","title":"Authentication","text":""},{"location":"authentication/#oauth2","title":"OAuth2","text":"<p>The booking service API uses OAuth2 as its authentication mechanism, providing a secure way for access control. The diagram below illustrates the flow of the OAuth2 authentication mechanism, showing how the client application interacts with the authorization and resource endpoints to authenticate and access protected resources.</p> <p> </p> <p> Diagram: Flow of OAuth2 authentication </p>"},{"location":"authentication/#oauth2-flow-in-the-booking-service-api","title":"OAuth2 Flow in the Booking Service API","text":""},{"location":"authentication/#requesting-a-token","title":"Requesting a Token","text":"<p>The <code>/token</code> endpoint handles login, where the user sends credentials using the <code>OAuth2PasswordRequestForm</code>. When valid credentials are provided, a JWT token is generated.</p> <pre><code>@router.post(\"/token\", response_model=Token)\nasync def login_for_access_token(form_data: OAuth2PasswordRequestForm, database_session: Session = Depends(get_database_session)) -&gt; Token:\n    user = authenticate_user(database_session, form_data.username, form_data.password)\n    if not user:\n        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Incorrect username or password\", headers={\"WWW-Authenticate\": \"Bearer\"})\n\n    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n    access_token = create_access_token(data={\"sub\": user.username, \"role\": user.role}, expires_delta=access_token_expires)\n    return Token(access_token=access_token, token_type=\"bearer\")\n</code></pre> <p>The form data is sent as:</p> <pre><code>Content-Type: application/x-www-form-urlencoded\nusername=your_username&amp;password=your_password\n</code></pre> <p>The <code>Token</code> model represents the JWT token returned after successful authentication:</p> <pre><code>class Token(BaseModel):\n    access_token: str\n    token_type: str\n</code></pre> <p>The <code>authenticate_user</code> function verifies the user\u2019s identity by retrieving their details from the database using <code>get_user</code> and validating the password with <code>verify_password</code>.</p> <pre><code>def authenticate_user(database_session: Session, username: str, password: str) -&gt; Union[UserInDB, bool]:\n    user = get_user(database_session, username)\n    if not user:\n        return False\n    if not verify_password(password, user.hashed_password):\n        return False\n    return user\n</code></pre> <p>The <code>create_access_token</code> function then generates the JWT token using the provided user information.</p> <pre><code>def create_access_token(data: Dict, expires_delta: Optional[timedelta] = None) -&gt; str:\n    to_encode = data.copy()\n    expire = datetime.now(timezone.utc) + (expires_delta or timedelta(minutes=15))\n    to_encode.update({\"exp\": expire})\n    encoded_jwt = jwt.encode(payload=to_encode, key=PRIVATE_KEY, algorithm=ALGORITHM)\n    return encoded_jwt\n</code></pre> <p>A JWT is an encoded string containing user information and an expiration time, signed using the digital signatures and the specified algorithm. A JWT is composed of three parts separated by dots. For example:</p> <pre><code>eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c\n</code></pre> <ul> <li>Header: Encodes metadata such as the signing algorithm (<code>eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9</code>).</li> <li>Payload: Contains claims like user ID and expiration time (<code>eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ</code>).</li> <li>Signature: A hashed combination of the header, payload, and the secret key (<code>SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c</code>).</li> </ul>"},{"location":"authentication/#retrieving-user-credentials-from-the-database","title":"Retrieving User Credentials from the Database","text":"<p>The <code>get_user</code> function accesses the database to retrieve the user record, which is then wrapped in a <code>UserInDB</code> model if the user exists.</p> <pre><code>def get_user(database_session: Session, username: str) -&gt; Optional[UserInDB]:\n    user = database_session.query(User).filter(User.username == username).first()\n    if user:\n        return UserInDB(username=user.username, hashed_password=user.hashed_password, disabled=user.disabled, role=user.role)\n    return None\n</code></pre> <p>The <code>UserInDB</code> model represents the user\u2019s details, including the username, hashed password, and role.</p> <pre><code>class UserDetails(BaseModel):\n    username: str\n    disabled: Optional[bool] = None\n    role: Optional[str] = None\n\nclass UserInDB(UserDetails):\n    hashed_password: str\n</code></pre> <p>The separation of <code>UserDetails</code> and <code>UserInDB</code> models distinguishes between the user details stored in the database (<code>UserInDB</code>) and the user details returned to the client (<code>UserDetails</code>), ensuring the hashed password remains secure.</p>"},{"location":"authentication/#validating-the-password","title":"Validating the Password","text":"<p>The <code>verify_password</code> function compares the plain text password provided during login to the hashed password stored in the database.</p> <pre><code>pwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n\ndef verify_password(plain_password: str, hashed_password: str) -&gt; bool:\n    return pwd_context.verify(plain_password, hashed_password)\n</code></pre> <p>This function uses the CryptContext class from <code>passlib</code> to validate the password securely.</p>"},{"location":"authentication/#retrieving-and-validating-the-current-user","title":"Retrieving and Validating the Current User","text":"<p>The <code>get_current_user</code> function decodes the JWT token via <code>jwt.decode</code> to extract the user\u2019s information (e.g., <code>username</code>). It validates the token's integrity and checks for expiration.</p> <pre><code>async def get_current_user(token: Annotated[str, Depends(oauth2_scheme)], database_session: Session = Depends(get_database_session)) -&gt; UserInDB:\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n        headers={\"WWW-Authenticate\": \"Bearer\"},\n    )\n    try:\n        payload = jwt.decode(jwt=token, key=PUBLIC_KEY, algorithms=[ALGORITHM])\n        username: Optional[str] = payload.get(\"sub\")\n        if username is None:\n            raise credentials_exception\n        token_data = TokenData(username=username)\n    except ExpiredSignatureError:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Token has expired\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    except InvalidTokenError:\n        raise credentials_exception\n    except Exception as error:\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=f\"Unexpected error: {str(error)}\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    user = get_user(database_session, username=token_data.username)\n    if user is None:\n        raise credentials_exception\n    return user\n</code></pre> <p>The <code>sub</code> key in the payload contains the username, which is used to retrieve the user from the database. If the user is not found, an unauthorized error is raised.</p>"},{"location":"authentication/#oauth2passwordbearer","title":"OAuth2PasswordBearer","text":"<p>The <code>oauth2_scheme</code> dependable is an instance of <code>OAuth2PasswordBearer</code>, a dependency provided by FastAPI for OAuth2 authentication. It is used to extract the Bearer token from the request's <code>Authorization</code> header.</p> <pre><code>oauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n</code></pre> <ul> <li>The parameter <code>tokenUrl=\"token\"</code> specifies the endpoint where the token can be obtained. In this case, the <code>/token</code> endpoint issues the JWT when a user provides valid credentials.</li> <li>When used in <code>Depends(oauth2_scheme)</code>, FastAPI automatically extracts and validates the token passed in the request. The token is then passed to the <code>get_current_user</code> function as a dependency.</li> </ul> <p>The <code>oauth2_scheme</code> ensures that each request to protected endpoints must include a valid token in the <code>Authorization</code> header (e.g., <code>Authorization: Bearer &lt;token&gt;</code>). Without a valid token, access to the endpoint will be denied, and an HTTP 401 Unauthorized error will be returned.</p> <p>An example of a request to a protected endpoint including the token:</p> <pre><code>curl -X 'POST' \\\n  'https://api.example.com/booking/' \\\n  -H 'accept: application/json' \\\n  -H 'Authorization: Bearer &lt;access_token_placeholder&gt;' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    ...\n  }'\n</code></pre> <p>The token, once extracted, is validated and decoded within <code>get_current_user</code> to authenticate the user and retrieve user details from the database.</p>"},{"location":"authentication/#tokendata-model","title":"TokenData Model","text":"<p>The <code>TokenData</code> model represents the token data, which includes the username:</p> <pre><code>class TokenData(BaseModel):\n    username: str\n</code></pre>"},{"location":"authentication/#verifying-user-activity-and-role","title":"Verifying User Activity and Role","text":"<ul> <li><code>get_current_active_user</code> ensures the user is active.</li> <li><code>get_current_admin</code> verifies if the user has an admin role.</li> <li><code>get_current_user_or_admin</code> checks if the user has either \"admin\" or \"requester\" roles.</li> </ul> <pre><code>async def get_current_active_user(current_user: Annotated[UserDetails, Depends(get_current_user)]) -&gt; UserDetails:\n    if current_user.disabled:\n        raise HTTPException(status_code=400, detail=\"Inactive user\")\n    return current_user\n</code></pre> <pre><code>async def get_current_admin(current_user: Annotated[UserDetails, Depends(get_current_user)]) -&gt; UserDetails:\n    if current_user.role != \"admin\":\n        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=\"Not enough permissions\")\n    return current_user\n</code></pre> <pre><code>async def get_current_user_or_admin(current_user: Annotated[UserDetails, Depends(get_current_user)]) -&gt; UserDetails:\n    if current_user.role not in [\"requester\", \"admin\"]:\n        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=\"Not enough permissions\")\n    return current_user\n</code></pre> <p>These functions use <code>get_current_user</code> as a dependency to validate the token and additionally check for specific conditions (e.g., if the user is active or has the required permissions).</p>"},{"location":"authentication/#accessing-protected-resources","title":"Accessing Protected Resources","text":"<p>For protected endpoints like <code>GET</code>, <code>POST</code>, and <code>DELETE</code>, these dependencies verify the user\u2019s identity and role before allowing access to the resource.</p> <pre><code>@router.post(\n    \"/\",\n    status_code=201,\n    response_model=BookingResponse,\n    dependencies=[Depends(get_current_user_or_admin), Depends(get_current_active_user)],\n)\n\n@router.get(\n    \"/\",\n    status_code=200,\n    response_model=BookingResponseList,\n    dependencies=[Depends(get_current_admin), Depends(get_current_active_user)],\n)\n\n@router.post(\n    \"/accept/\",\n    status_code=200,\n    response_model=BookingResponse,\n    dependencies=[Depends(get_current_admin), Depends(get_current_active_user)],\n)\n\n@router.post(\n    \"/reject/\",\n    status_code=200,\n    response_model=BookingResponse,\n    dependencies=[Depends(get_current_admin), Depends(get_current_active_user)],\n)\n\n@router.delete(\n    \"/{id}/\",\n    status_code=200,\n    response_model=BookingResponse,\n    dependencies=[Depends(get_current_admin), Depends(get_current_active_user)],\n)\n</code></pre>"},{"location":"authentication/#digital-signature-algorithms-in-the-booking-service-api","title":"Digital Signature Algorithms in the Booking Service API","text":"<p>The booking service API uses EdDSA (Edwards-curve Digital Signature Algorithm) for signing and verifying JWT tokens. This algorithm uses the Ed25519 curve, which offers 128-bit security.</p>"},{"location":"authentication/#asymmetric-public-key-cryptography","title":"Asymmetric (Public-Key) Cryptography","text":"<p>Asymmetric cryptography involves a pair of keys: a private key used for signing (encoding) and a public key used for verifying (decoding).</p> <ul> <li><code>Ed25519</code>: This is the algorithm used for signing and verifying tokens. </li> <li>The private key is securely stored in AWS Secrets Manager and used in <code>create_access_token</code> to sign tokens.</li> <li>The public key is also stored in AWS Secrets Manager and is used in <code>get_current_user</code> to verify the authenticity of tokens.</li> </ul>"},{"location":"authentication/#code-implementation-highlights","title":"Code Implementation Highlights","text":""},{"location":"authentication/#setting-up-the-environment-and-keys","title":"Setting Up the Environment and Keys","text":"<pre><code>ALGORITHM = \"EdDSA\"\nENV = os.getenv(\"ENV\")\n\n# In test mode, we don't need to fetch the secrets from aws secrets manager since we mock the authentication\nif ENV == \"test\":\n    PUBLIC_KEY, PRIVATE_KEY = \"test_public_key\", \"test_private_key\"\n# In dev or prod mode, fetch the secrets from aws secrets manager\nelif ENV in [\"dev\", \"prod\"]:\n    sm = boto3.client(\"secretsmanager\")\n    # Secret and algorithm settings\n    PUBLIC_KEY = b64decode(\n        sm.get_secret_value(SecretId=f\"public_key_{ENV}\")[\"SecretString\"]\n    ).decode(\"utf-8\")\n    PRIVATE_KEY = b64decode(\n        sm.get_secret_value(SecretId=f\"private_key_{ENV}\")[\"SecretString\"]\n    ).decode(\"utf-8\")\n    sm.close()\nelse:\n    raise RuntimeError(\"Unknown environment: Please set 'ENV' to 'test', 'dev', or 'prod'\")\n</code></pre> <p>The <code>PUBLIC_KEY</code> and <code>PRIVATE_KEY</code> are fetched from AWS Secrets Manager in <code>dev</code> and <code>prod</code> environments. In <code>test</code> mode, the keys are set to dummy values for testing purposes. This is because integration tests involving protected endpoints use mocked versions of the authentication functions; therefore, no real keys are needed.</p>"},{"location":"authentication/#signing-and-verifying-tokens","title":"Signing and Verifying Tokens","text":"<ul> <li> <p>Signing (using the private key):</p> <pre><code>encoded_jwt = jwt.encode(payload=to_encode, key=PRIVATE_KEY, algorithm=ALGORITHM)\n</code></pre> </li> <li> <p>Verifying (using the public key):</p> <pre><code>payload = jwt.decode(jwt=token, key=PUBLIC_KEY, algorithms=[ALGORITHM])\n</code></pre> </li> </ul> <p>Both operations use <code>EdDSA</code> with the Ed25519 curve to ensure secure signing and verification.</p>"},{"location":"authentication/#generating-and-storing-ed25519-keys-relevant-script-lines","title":"Generating and Storing Ed25519 Keys (Relevant Script Lines)","text":"<p>The <code>scripts/generate_and_store_keys.sh</code> script generates Ed25519 keys, encodes them in base64, and stores them in AWS Secrets Manager. The secrets resources are provisioned in the <code>infrastructure/modules/booking_service/api_keys_creds.tf</code> Terraform configurtation file.</p> <ol> <li> <p>Generating Keys:</p> <pre><code>openssl genpkey -algorithm Ed25519 -out \"$PRIVATE_KEY_FILE\"\nopenssl pkey -in \"$PRIVATE_KEY_FILE\" -pubout -out \"$PUBLIC_KEY_FILE\"\n</code></pre> </li> <li> <p>Base64 Encoding:</p> <pre><code>PRIVATE_KEY=$(base64 &lt; \"$PRIVATE_KEY_FILE\")\nPUBLIC_KEY=$(base64 &lt; \"$PUBLIC_KEY_FILE\")\n</code></pre> </li> <li> <p>Storing in AWS Secrets Manager:</p> <pre><code>aws secretsmanager put-secret-value --secret-id \"private_key_${ENV}\" --secret-string \"$PRIVATE_KEY\" --profile \"$AWS_PROFILE\"\naws secretsmanager put-secret-value --secret-id \"public_key_${ENV}\" --secret-string \"$PUBLIC_KEY\" --profile \"$AWS_PROFILE\"\n</code></pre> </li> </ol> <p>These lines generate, encode, and store Ed25519 keys in AWS Secrets Manager so they can be securely accessed by the booking service API at runtime.</p>"},{"location":"migrations/","title":"Migrations","text":""},{"location":"migrations/#directory-structure","title":"Directory Structure","text":"<pre><code>.\n\u251c\u2500\u2500 README\n\u251c\u2500\u2500 alembic.ini\n\u251c\u2500\u2500 env.py\n\u251c\u2500\u2500 script.py.mako\n\u2514\u2500\u2500 versions\n    \u251c\u2500\u2500 00ba29ba9ef0_create_user_table.py\n    \u2514\u2500\u2500 be5c4de9546c_initial_migration.py\n</code></pre> <ul> <li>alembic.ini: The main Alembic configuration file where settings, including the database URL and migration paths, are defined.</li> <li>env.py: The script Alembic uses to set up the migration environment, configuring the database connection and autogenerate support for models.</li> <li>script.py.mako: A template file used for generating migration scripts.</li> <li>versions/: Directory containing versioned migration scripts. Each migration script tracks specific changes to the database schema.</li> </ul>"},{"location":"migrations/#alembic-configuration-alembicini","title":"Alembic Configuration (<code>alembic.ini</code>)","text":"<p>The <code>alembic.ini</code> file configures how Alembic connects to the database and how migration scripts are generated. This file can exist in any directory, with the location to it specified by either the <code>--config</code> option for the <code>alembic</code> runner or the <code>ALEMBIC_CONFIG</code> environment variable. Key settings include:</p> Setting Description <code>script_location</code> Points to the directory where migration scripts are stored (<code>migrations/versions</code>). <code>sqlalchemy.url</code> Defines the database connection string, used when running migrations (<code>driver://user:pass@localhost/dbname</code>). This can be overridden dynamically in <code>env.py</code> using <code>get_settings()</code>. <code>version_path_separator</code> Determines the separator used for multiple version paths (default is based on the operating system). <code>[loggers]</code>, <code>[handlers]</code>, <code>[formatters]</code> Logging configuration for Alembic, enabling detailed logs during migration operations."},{"location":"migrations/#environment-setup-envpy","title":"Environment Setup (<code>env.py</code>)","text":"<p>The <code>env.py</code> is the core of Alembic\u2019s migration environment, setting up connections and managing how migrations are applied. It handles two scenarios: offline and online migration modes.</p> <ol> <li> <p>Configuration Setup:</p> <ul> <li>The configuration object (<code>config</code>) reads values from <code>alembic.ini</code>.</li> <li>Logging configuration is set up using <code>fileConfig(config.config_file_name)</code>.</li> </ul> </li> <li> <p>Target Metadata:</p> <ul> <li><code>target_metadata</code> is assigned the metadata from SQLAlchemy models (<code>Base.metadata</code>), enabling autogeneration of migrations based on ORM models.</li> </ul> <pre><code>from app.models.db_models import Base\ntarget_metadata = [Base.metadata]\n</code></pre> </li> <li> <p>Offline Migrations (<code>run_migrations_offline</code>):</p> <ul> <li>Runs migrations without an active database connection. Suitable for generating SQL scripts.</li> <li>Uses <code>context.execute()</code> to emit SQL directly.</li> </ul> <pre><code>def run_migrations_offline() -&gt; None:\n    url = config.get_main_option(\"sqlalchemy.url\", get_settings().database_url)\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n    with context.begin_transaction():\n        context.run_migrations()\n</code></pre> </li> <li> <p>Online Migrations (<code>run_migrations_online</code>):</p> <ul> <li>Creates an SQLAlchemy engine and associates it with the migration context.</li> <li>It fetches the database URL dynamically from the application\u2019s settings using <code>get_settings()</code> to ensure the environment is correctly set (<code>dev</code>, <code>test</code>, <code>prod</code>).</li> </ul> <pre><code>def run_migrations_online() -&gt; None:\n    alembic_config = config.get_section(config.config_ini_section, {})\n    alembic_config[\"sqlalchemy.url\"] = get_settings().database_url\n    connectable = engine_from_config(\n        alembic_config,\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n    with connectable.connect() as connection:\n        context.configure(connection=connection, target_metadata=target_metadata)\n        with context.begin_transaction():\n            context.run_migrations()\n</code></pre> </li> </ol>"},{"location":"migrations/#sqlalchemy-orms","title":"SQLAlchemy ORMs","text":"<p>The database models are defined using SQLAlchemy ORM, which maps Python classes to database tables. Alembic detects changes to these models for migration generation.</p>"},{"location":"migrations/#declarative-base","title":"Declarative Base","text":"<pre><code>class Base(DeclarativeBase):\n    pass\n</code></pre>"},{"location":"migrations/#user-model","title":"User Model","text":"<p>The <code>User</code> model stores credentials and user information:</p> <pre><code>class User(Base):\n    __tablename__ = \"users\"\n    id = mapped_column(Integer, primary_key=True, autoincrement=True)\n    username = mapped_column(String(100), unique=True, nullable=False)\n    hashed_password = mapped_column(String, nullable=False)\n    disabled = mapped_column(Boolean, default=False)\n    role = mapped_column(String(50), nullable=False)\n</code></pre>"},{"location":"migrations/#booking-model","title":"Booking Model","text":"<p>The <code>Booking</code> model stores booking information:</p> <pre><code>class Booking(Base):\n    __tablename__ = \"booking_requests\"\n    id: Mapped[Integer] = mapped_column(\n        Integer, primary_key=True, index=True, nullable=False, autoincrement=True\n    )\n    event_time: Mapped[DateTime] = mapped_column(DateTime, nullable=True)\n    address: Mapped[JSON] = mapped_column(JSON, nullable=False)\n    duration_minutes: Mapped[SmallInteger] = mapped_column(SmallInteger, nullable=False)\n    topic: Mapped[String] = mapped_column(String, nullable=False)\n    requested_by: Mapped[String] = mapped_column(String(100), nullable=False)\n    status: Mapped[String] = mapped_column(String(10), nullable=False)\n</code></pre> <p>Alembic uses the <code>Base.metadata</code> attribute to detect changes when generating migrations. Whenever a model is added or modified, a new migration script can be generated.</p>"},{"location":"migrations/#declarative-vs-imperative-classical-mapping","title":"Declarative vs. Imperative (Classical) Mapping","text":"<p>SQLAlchemy offers two ORM mapping styles: Declarative and Imperative (Classical). Both styles are valid and the internal process of mapping the classes is mostly the same.</p> <ul> <li> <p>Declarative Mapping:</p> <ul> <li>This is the modern and most tpyical approach in SQLAlchemy. It involves subclassing a base class (<code>DeclarativeBase</code>) where models define their table structure and metadata directly within the class.</li> <li>Declarative mapping is intuitive and integrates well with modern Python typing (e.g., PEP 484 support), making it suitable for most applications.</li> <li>It automates much of the configuration process, making it simpler and cleaner for developers to define ORM models.</li> </ul> </li> <li> <p>Imperative (Classical) Mapping:</p> <ul> <li>This is a lower-level and less commonly used approach that predates the declarative system. It involves creating table metadata separately and then associating it with classes using the registry.map_imperatively() method.</li> <li>While it provides a more barebones and flexible way to define mappings, it lacks some modern features like integration with Python type checkers.</li> <li>It is often used when developers need explicit control over the table-to-class mapping process, or when integrating with legacy systems that might not follow the declarative pattern.</li> </ul> </li> </ul>"},{"location":"migrations/#dependencies-and-configuration-dbpy-and-configpy","title":"Dependencies and Configuration (<code>db.py</code> and <code>config.py</code>)","text":"<ul> <li> <p><code>db.py</code>: This module provides the connection and session management using SQLAlchemy. It utilizes the application\u2019s settings to dynamically set up database connection strings.</p> <pre><code>def get_local_session() -&gt; sessionmaker:\n    settings = get_settings()\n    database_url = settings.database_url\n    engine = create_engine(database_url)\n    session_local = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n    return session_local\n</code></pre> <pre><code>def get_database_session() -&gt; Generator[Session, None, None]:\n    session_local = get_local_session()\n    database_session = session_local()\n    try:\n        yield database_session\n    finally:\n        database_session.close()\n</code></pre> </li> <li> <p><code>config.py</code>: Configuration management module using Pydantic for structured settings. It dynamically fetches database connection strings from AWS Secrets Manager, ensuring that environment-specific configurations are correctly applied.</p> <pre><code>class BaseAppSettings(BaseSettings):\n    environment: str\n    _database_url: Optional[str] = None\n\n    @property\n    def database_url(self) -&gt; str:\n        if self._database_url is None:\n            sm = boto3.client(\"secretsmanager\")\n            self._database_url = sm.get_secret_value(SecretId=f\"db_connection_string_{self.environment}\")[\"SecretString\"]\n        return self._database_url\n</code></pre> <pre><code>@lru_cache()\ndef get_settings() -&gt; BaseAppSettings:\n    env = os.getenv(\"ENV\", None)\n    match env:\n        case \"dev\":\n            return BaseAppSettings(environment=\"dev\", debug=True, testing=False)\n        case \"prod\":\n            return BaseAppSettings(environment=\"prod\", debug=False, testing=False)\n        case \"test\":\n            return TestSettings(environment=\"test\", debug=True, testing=True)\n        case _:\n            raise ValueError(f\"Invalid ENV environment variable: {env}\")\n</code></pre> </li> </ul> <p>This dynamic setup allows <code>env.py</code> to fetch the correct database URL based on the environment, ensuring consistent configurations during migrations.</p>"},{"location":"migrations/#migration-scripts-versions-directory","title":"Migration Scripts (<code>versions</code> Directory)","text":"<p>Each migration script captures specific changes made to the database schema:</p> <ul> <li> <p>Naming: The script name starts with a revision ID (e.g., <code>00ba29ba9ef0</code>) and includes a slug for clarity (<code>create_user_table</code>).</p> </li> <li> <p>Structure:</p> <ul> <li><code>upgrade</code> function: Contains SQL statements or ORM-based changes to upgrade the schema.</li> <li><code>downgrade</code> function: Reverses changes made by <code>upgrade</code>, allowing rollback of migrations.</li> </ul> <pre><code>def upgrade() -&gt; None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"users\",\n        sa.Column(\"id\", sa.Integer(), autoincrement=True, nullable=False),\n        sa.Column(\"username\", sa.String(length=100), nullable=False),\n        sa.Column(\"hashed_password\", sa.String(), nullable=False),\n        sa.Column(\"disabled\", sa.Boolean(), nullable=False),\n        sa.Column(\"role\", sa.String(length=50), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_users_id\"), \"users\", [\"id\"], unique=False)\n    op.create_index(op.f(\"ix_users_username\"), \"users\", [\"username\"], unique=True)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -&gt; None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_users_username\"), table_name=\"users\")\n    op.drop_index(op.f(\"ix_users_id\"), table_name=\"users\")\n    op.drop_table(\"users\")\n    # ### end Alembic commands ###\n</code></pre> </li> </ul>"},{"location":"migrations/#migration-runner-commands","title":"Migration Runner Commands","text":"<p>Running the commands within a service container using docker compose exec:</p> <ul> <li>Initialize Alembic (First Time):</li> </ul> <pre><code>$ docker compose exec &lt;service-name&gt; alembic -c migrations/alembic.ini init migrations\n</code></pre> <ul> <li>Generate Migration:</li> </ul> <pre><code>$ docker compose exec &lt;service-name&gt; alembic -c migrations/alembic.ini revision --autogenerate -m \"describe change\"\n</code></pre> <ul> <li>Apply Migration:</li> </ul> <pre><code>$ docker compose exec &lt;service-name&gt; alembic -c migrations/alembic.ini upgrade head\n</code></pre> <ul> <li>Rollback Migration:</li> </ul> <pre><code>$ docker compose exec &lt;service-name&gt; alembic -c migrations/alembic.ini downgrade -1\n</code></pre>"},{"location":"migrations/#sqlalchemy-dialect-and-driver-compatibility","title":"SQLAlchemy Dialect and Driver Compatibility","text":"<p>It's crucial that the SQLAlchemy driver matches the database connection string format. In this project, the <code>psycopg3</code> driver is used for PostgreSQL, with the connection string structured as follows:</p> <pre><code>postgresql+psycopg://user:password@host:port/dbname\n</code></pre> <p>In aws, the database connection string is stored in AWS Secrets Manager:</p> <pre><code>postgresql+psycopg://${aws_db_instance.booking_service.username}:${urlencode(random_password.db_password.result)}@${aws_db_instance.booking_service.endpoint}/${aws_db_instance.booking_service.db_name}\n</code></pre> <p>Note the use of urlencode to encode the password, ensuring that special characters are correctly handled.</p> <ul> <li> <p>Synchronous (Used in this Project):</p> <pre><code>from sqlalchemy import create_engine\nsync_engine = create_engine(\"postgresql+psycopg://user:password@localhost/dbname\")\n</code></pre> </li> <li> <p>Asynchronous:</p> <pre><code>from sqlalchemy.ext.asyncio import create_async_engine\nasyncio_engine = create_async_engine(\"postgresql+psycopg://user:password@localhost/dbname\")\n</code></pre> </li> </ul> <p>The choice of synchronous or asynchronous operation depends on each application's needs. For projects requiring high concurrency and non-blocking operations, the asynchronous engine is appropriate.</p> <p>For more details on configuring psycopg with SQLAlchemy, see the official documentation: SQLAlchemy Documentation.</p>"},{"location":"setup_workflows/","title":"Set Up Workflows","text":""},{"location":"setup_workflows/#environment","title":"Environment","text":""},{"location":"setup_workflows/#install-pdm","title":"Install PDM","text":"<p>The dependency manager used in this project is pdm. To install it, run the following command:</p> <pre><code>$ curl -sSL https://pdm-project.org/install-pdm.py | python3 -\n</code></pre> <p>Or, alternatively, other installation methods can be used.</p>"},{"location":"setup_workflows/#install-dependencies","title":"Install Dependencies","text":"<p>The dependencies are broken into groups:</p> <ul> <li> <p>Default dependencies: required for the core functionality of the project in production.</p> </li> <li> <p>Development dependencies: required for development, testing, and documentation.</p> </li> </ul> <p>The specified python version in <code>pyproject.toml</code> is <code>&gt;=3.11</code>, and so a python 3.11 interpreter should be used. </p>"},{"location":"setup_workflows/#conda","title":"Conda","text":"<p>To do so with conda:</p> <pre><code>$ conda search python | grep \" 3\\.\\(10\\|11\\|12\\)\\.\"\n$ yes | conda create --name booking_service_api python=3.11.9\n$ conda activate booking_service_api\n$ pdm use -f $(which python3)\n$ pdm install\n</code></pre>"},{"location":"setup_workflows/#vitualenv","title":"Vitualenv","text":"<p>To do so with virtualenv, use the pdm venv command:</p> <pre><code>$ pdm venv create --name booking_service_api --with virtualenv 3.11.9 \n# To activate the virtual environment\n$ eval $(pdm venv activate booking_service_api) \n$ pdm install\n</code></pre>"},{"location":"setup_workflows/#docker-compose-for-local-development","title":"Docker Compose for Local Development","text":"<p>Docker Compose is used for local development to isolate the application and its dependencies in a containerized environment. Two services are defined the the <code>compose.yml</code> file:</p> <ol> <li> <p><code>test</code>: The service for running the application, unit, integration, and end-to-end tests. The following directories are mapped or bind-mounted from the host to the container:</p> <ul> <li><code>app/**</code>: The application code to run the FastAPI application.</li> <li><code>tests/**</code>: The test code as all tests are run in the container.</li> <li><code>pyproject.toml</code>: The project configuration file, which includes the dependencies, pytest configurations, and other project settings.</li> <li><code>migrations</code>: The Alembic migrations directory to manage the database schema. This is mounted to the container to apply migrations each time a new container is created (i.e., <code>docker compose up --detach --build</code>).</li> </ul> </li> <li> <p><code>db-test</code>: The service for running the PostgreSQL database for testing purposes. The database is initialized with the schema and data required for testing based on the migration files in the <code>migrations</code> directory.</p> </li> </ol> <p>The entrypoing script of the <code>test.Dockerfile</code> waits for the PostgreSQL database to start before running the application. This is done by checking if the port <code>5432</code> is open using the <code>nc</code> command. The script is as follows:</p> <pre><code>#!/bin/sh\n\necho \"Waiting for postgres to start...\"\n\nwhile ! nc -z db-test 5432; do\n  # The nc (Netcat) is utility to check network connectivity\n  # The -z flag ensures nc only scan for open ports without sending data\n  # If the connection fails (i.e., PostgreSQL isn't up yet), the loop continues\n  sleep 0.1\ndone\n\necho \"PostgreSQL started\"\n\n# 'exec' replaces the current shell process with the command and arguments passed to the script, preserving all arguments as separate arguments\n# For example: ./script.sh command --option value -&gt; exec command --option value\nexec \"$@\"\n</code></pre>"},{"location":"setup_workflows/#build-and-run-the-containers","title":"Build and Run the Containers","text":"<p>To build the images and run the containers in the background:</p> <pre><code>$ docker compose up --detach --build\n</code></pre> <p>This setup allows for automatic reloading of the application when changes are made to the code during development. The application is available at http://localhost:8004 or whichever port is specified in the <code>compose.yml</code> file.</p> <p>To stop the containers without removing them:</p> <pre><code>$ docker compose stop\n</code></pre> <p>To stop, remove the containers, and remove named volumes:</p> <pre><code>$ docker compose down --volumes\n</code></pre> <p>To view the logs of the services:</p> <pre><code>$ docker compose logs &lt;service-name&gt;\n</code></pre> <p>To run an interactive shell in a service container:</p> <pre><code># Or /bin/bash\n$ docker compose exec &lt;service-name&gt; /bin/sh\n</code></pre>"},{"location":"setup_workflows/#testing-with-pytest","title":"Testing with Pytest","text":"<p>The test suite is run using pytest and is divided into three categories:</p> <p> Test Type Description Unit Tests Validates individual components like models, services, and utility functions. Integration Tests Tests the integration between database services and API endpoints. End-to-End Tests Simulates real user flows, ensuring the entire application functions as expected. <p></p> <p>The integration tests are run against a PostgreSQL database running in a Docker container. The <code>DATABASE_URL_TEST</code> database connection string is set as an environment variable in the <code>compose.yml</code> file.</p> <pre><code>$ docker compose exec &lt;service-name&gt; python3 -m pytest -s tests/integration tests/unit -v\n</code></pre> <p>The end-to-end tests are run against the FastAPI application running in <code>dev</code> mode on aws. The <code>.github/workflows/ci_cd_end_to_end.yml</code> workflow is configured to run after the <code>.github/workflows/ecr_ecs_dev.yml</code> workflow completes successfully. It sets up the aws cli and fetches the authentication credentials from the aws secrets manager to run the end-to-end tests.</p>"},{"location":"setup_workflows/#automation","title":"Automation","text":""},{"location":"setup_workflows/#formatting-with-black-and-isort","title":"Formatting with Black and Isort","text":"<p>The <code>scripts</code> directory contains a <code>run_black_isort.sh</code> shell script that runs black and isort on the project files. The script is also used in the GitHub Actions workflows to ensure consistent code formatting. </p> <p>Run the script as follows:</p> <pre><code>$ cd services/booking\n$ scripts/run_black_isort.sh\n</code></pre>"},{"location":"terraform/","title":"Terraform","text":"<p>The infrastructure for the booking service api is managed using Terraform. The Terraform configuration files are located in the <code>infrastucture</code> directory.</p>"},{"location":"terraform/#installation","title":"Installation","text":"<p>Install Terraform command line interface by following the instructions in the official documentation:</p> <pre><code>$ terraform --version\n</code></pre>"},{"location":"terraform/#terraform-terminologies","title":"Terraform Terminologies","text":"<ul> <li> <p>Configuration Files: Files with <code>.tf</code> extension, must use UTF-8 encoding.</p> </li> <li> <p>Module: A collection of <code>.tf</code> files in a directory, can be local or remote.</p> </li> <li> <p>Lock File: Terraform selects dependency versions based on configuration-defined version constraints. The dependency lock file (<code>.terraform.lock.hcl</code>) records selected provider versions for consistency across runs. Module versions are not tracked; the latest version meeting constraints is selected unless explicitly defined. Terraform automatically creates or updates this file each time the <code>terraform init</code> command is run against the current working directory. It should be included in version control to facilitate discussions on external dependencies via code review, similar to configuration changes.</p> </li> <li> <p>Cache Directory: Terraform uses a cache directory (<code>.terraform</code>) to store various items. This directory should be included in <code>.gitignore</code> files to prevent unnecessary secrets from being committed to version control.</p> </li> </ul>"},{"location":"terraform/#required-provider","title":"Required Provider","text":"<p>The required provider is hashicorp/aws with version ~&gt; 5.0.</p>"},{"location":"terraform/#backend-configuration","title":"Backend Configuration","text":"<p>The Terraform state file is stored in an S3 bucket using the s3 backend. </p> <pre><code>terraform {\n  backend \"s3\" {\n    bucket  = \"s3-bucket-name\"\n    key     = \"path/to/terraform.tfstate\"\n  }\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n  }\n}\n</code></pre>"},{"location":"terraform/#purpose-of-terraform-state","title":"Purpose of Terraform State","text":"<ul> <li>Mapping Resources: State maps each resource in the configuration to its real-world equivalent (e.g., an AWS instance ID). This one-to-one mapping prevents ambiguity and errors.</li> <li>Metadata Tracking: State retains dependencies and provider metadata, allowing Terraform to manage resource creation and destruction order, even when resources are deleted from the configuration.</li> <li>Performance Optimization: State caches attribute values, reducing the need to query all resources during each <code>terraform plan</code> or <code>apply</code>, which is crucial for large infrastructures to minimize API rate limits and response times.</li> <li>Syncing and Remote State: Local state files work for initial setups, but remote state is recommended for collaborative projects to ensure consistency and enable state locking, preventing simultaneous runs and maintaining up-to-date information.</li> </ul>"},{"location":"terraform/#infrastructure","title":"Infrastructure","text":""},{"location":"terraform/#global-infrastructure","title":"Global Infrastructure","text":"<ol> <li> <p>IAM Module:</p> <ul> <li> <p>Sets up a GitHub Actions role using OpenID Connect (OIDC) for automating subsequent deployments via workflows.</p> </li> <li> <p>This module is managed manually (i.e., <code>terraform init</code>, <code>terraform validate</code>, <code>terraform plan</code>, and <code>terraform apply</code> are executed locally instead of using GitHub-hosted runners) to assign the AdministratorAccess managed policy to the role assumed by the github runner. The permissions can be refined further to follow the principle of least privilege, ensuring only necessary access is granted.</p> </li> </ul> </li> <li> <p>VPC Module:</p> <ul> <li> <p>Domain: Creates a (public) Route53 hosted zone for domain management.</p> </li> <li> <p>Network: Configures the VPC, including public/private subnets, route tables, internet gateway, and NAT gateway. VPC endpoints for services like S3 and ECR are also set up.</p> </li> <li> <p>Security Groups: Defines security groups for different components like the Application Load Balancer (ALB) and ECS tasks.</p> </li> </ul> </li> </ol>"},{"location":"terraform/#application-infrastructure-modulesbooking-service","title":"Application Infrastructure (Modules/Booking Service)","text":"<ol> <li> <p>Secrets Management:</p> <ul> <li>Manages digital signature keys, passwords, and other sensitive information using AWS Secrets Manager.</li> </ul> </li> <li> <p>ECR:</p> <ul> <li>Sets up an Elastic Container Registry (ECR) with lifecycle policies to manage image retention and expiration.</li> </ul> </li> <li> <p>ECS Cluster and Services:</p> <ul> <li>Configures ECS clusters with Fargate and Fargate Spot capacity providers.</li> <li>Defines ECS task definitions and services, ensuring proper role assignments and container configurations.</li> </ul> </li> <li> <p>Load Balancer:</p> <ul> <li>Deploys an ALB with HTTP and HTTPS listeners, target groups, and health check configurations.</li> </ul> </li> <li> <p>RDS:</p> <ul> <li>Sets up an RDS instance (running postgres 16.3) for the application, including database subnet groups and security groups.</li> <li>Generates a connection string stored in AWS Secrets Manager for secure access.</li> </ul> </li> <li> <p>Logging:</p> <ul> <li>Configures CloudWatch log groups and streams for ECS task logging.</li> </ul> </li> </ol>"},{"location":"terraform/#github-action-workflows","title":"Github Action Workflows","text":"<ol> <li> <p>Terraform Deployment Workflows:</p> <ul> <li> <p>terraform_vpc.yml: Deploys the VPC module.</p> </li> <li> <p>terraform_dev.yml and terraform_prod.yml: Deploy the development and production environments for the booking service using the predefined infrastructure modules. These workflows are mirror images of each other, with resources appropriately tagged or prefixed to differentiate between environments. This approach ensures that the development environment mirrors the production setup, enabling thorough testing and validation before deploying to production, minimizing risks and inconsistencies.</p> </li> </ul> </li> <li> <p>Reusable Workflow:</p> <ul> <li> <p>terraform_validate_plan_apply.yml: A reusable workflow that is invoked by all Terraform deployment workflows (e.g., VPC, dev, and prod) following the guidelines for reusing workflows. It standardizes the deployment process with consistent steps, i.e., <code>terraform init</code>, <code>terraform validate</code>, <code>terraform plan</code>, and <code>terraform apply</code>, across all environments.</p> </li> <li> <p>terraform_destroy.yml: A separate workflow used for teardown. It is manually triggered using workflow_dispatch and not part of the reusable pattern applied to the other workflows.</p> </li> </ul> </li> </ol>"},{"location":"terraform/#deployment-order","title":"Deployment Order","text":"<ol> <li> <p>GitHub Actions IAM Role: The IAM role using OpenID Connect (OIDC) must be created first, as it enables automated deployments via GitHub Actions workflows for subsequent infrastructure modules.</p> </li> <li> <p>VPC Module: The VPC sets up the network components required for the application infrastructure. The outputs from this module are referenced by the booking service modules (e.g., subnet IDs and security groups).</p> </li> <li> <p>Booking Service Modules: The application infrastructure components such as ECS, ECR, Secrets, and RDS are deployed last since they rely on the VPC\u2019s outputs.</p> </li> </ol>"},{"location":"terraform/#repository-secrets","title":"Repository Secrets","text":"<p>The following secrets must be configured in the GitHub repository:</p>"},{"location":"terraform/#aws-infrastructure-configuration-outputs-from-vpc-module","title":"AWS Infrastructure Configuration (Outputs from VPC Module)","text":"<ul> <li> <p>AWS_ECS_SECURITY_GROUP_ID: The ID of the security group assigned to the AWS ECS service, controlling inbound and outbound traffic for application containers and services such as database migrations.</p> </li> <li> <p>AWS_PRIVATE_SUBNET_1_ID &amp; AWS_PRIVATE_SUBNET_2_ID: The IDs of the private subnets used for deploying ECS tasks. These subnets provide a secure, isolated  environment for running application containers and managing standalone jobs like database migrations and user credential updates.</p> </li> </ul>"},{"location":"terraform/#github-actions-integration","title":"GitHub Actions Integration","text":"<ul> <li> <p>AWS_GITHUB_ACTIONS_ROLE_ARN: The Amazon Resource Name (ARN) for the IAM role that allows GitHub Actions to securely interact with AWS services using OpenID Connect (OIDC).</p> </li> <li> <p>AWS_REGION: The AWS region where resources (e.g., ECS, RDS, subnets) are hosted, ensuring deployments and container operations are executed in the correct regional context.</p> </li> </ul>"},{"location":"terraform/#testing-and-code-coverage","title":"Testing and Code Coverage","text":"<ul> <li>CODECOV_TOKEN: The token required for integrating Codecov into the CI/CD pipeline to report test coverage results securely. For more information, see the Codecov GitHub Action documentation.</li> </ul>"}]}